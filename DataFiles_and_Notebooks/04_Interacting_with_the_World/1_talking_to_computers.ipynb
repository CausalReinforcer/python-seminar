{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python Seminar (AY250) UC Berkeley\n",
    "\n",
    "# Super simple webpage access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run ../00_AdvancedPythonConcepts/talktools.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# URL = Uniform Resource Locator\n",
    "try:\n",
    "    # For Python 3.0 and later\n",
    "    from urllib.request import urlopen\n",
    "except ImportError:\n",
    "    # Fall back to Python 2's urllib2\n",
    "    from urllib2 import urlopen\n",
    "    \n",
    "# Brain maps data \"Explore the Brain like never before\"\n",
    "url = \"http://brainmaps.org/\"  \n",
    "response = urlopen(url) # response is a file-like object\n",
    "html_data = response.read()\n",
    "response.close() # close response as you would a normal file\n",
    "print(html_data[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small aside: if you have HTML data that you want to render, you can use `webbrowser` module\n",
    "\n",
    "see http://docs.python.org/library/webbrowser.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "open(\"/tmp/tmp.html\",\"w\").write(html_data.decode(\"UTF-8\"))\n",
    "webbrowser.open(\"file:///tmp/tmp.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scripting an HTTP GET request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from urllib.parse import urlencode\n",
    "except:\n",
    "    from urllib import urlencode\n",
    "\n",
    "\n",
    "# create a dictionary to store the GET data\n",
    "get_info = {\"q\": \"Joshua S. Bloom\", \"page\": \"2\"} \n",
    "\n",
    "# encode the data in proper URL format\n",
    "url_values = urlencode(get_info) \n",
    "print(url_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = \"http://pubget.com/search\"\n",
    "\n",
    "# open the url as before\n",
    "#alternatively: urlopen(url + \"?\" + url_values.encode(\"utf-8\"))\n",
    "response = urlopen(url,data=url_values.encode(\"utf-8\"))  \n",
    "\n",
    "html = response.read()\n",
    "response.close()\n",
    "print(html[8000:9000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scripting an HTTP POST request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = {}\n",
    "data[\"author\"] = \"Sagan, Carl\"\n",
    "params = urlencode(data).encode(\"UTF-8\") # same urlencode method\n",
    "url = \"http://adsabs.harvard.edu/cgi-bin/nph-abs_connect\"\n",
    "response = urlopen(url, params) \n",
    "# POST request is indicated by including the params in urlopen\n",
    "html = response.read()\n",
    "response.close()\n",
    "print(html[16474:19000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Access an FTP server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ftplib\n",
    "ftp = ftplib.FTP(\"ftp.cdc.gov\")\n",
    "ftp.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ftp.cwd(\"/pub/OPD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ftp.dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ftp.cwd(\"Susanna\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ftp.dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ftp.retrbinary('RETR SIKA_BANNER_7X3_reduced.pdf', open('zika.pdf', 'wb').write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pwd = !pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "\n",
    "webbrowser.open_new('file://{}/zika.pdf'.format(pwd[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing HTML with BeautifulSoup\n",
    "\n",
    "See: http://www.crummy.com/software/BeautifulSoup/bs4/doc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # For Python 3.0 and later\n",
    "    from urllib.request import urlopen\n",
    "except ImportError:\n",
    "    # Fall back to Python 2's urllib2\n",
    "    from urllib2 import urlopen\n",
    "    \n",
    "response = urlopen(\"http://words.bighugelabs.com/\")\n",
    "html = response.read()\n",
    "response.close()\n",
    "\n",
    "# pip install beautifulsoup4\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html,\"html.parser\")\n",
    "forms = soup.findAll(\"form\")\n",
    "forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "links = soup.findAll(\"form\")\n",
    "for link in links:\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load up a whole bunch of baby names, by combining scripted webpage access with BeautifulSoup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "url = \"http://nameberry.com/search/boys_names/J\"\n",
    "response = urlopen(url)\n",
    "html = response.read()\n",
    "response.close()\n",
    "soup = BeautifulSoup(html,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "items = soup.findAll(\"li\", class_=\"name_in_list\")\n",
    "print(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "items[35].a.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "letters = \"qwertyuioplkjhgfdsazxcvbnm\"\n",
    "boy_names = []\n",
    "for n in string.ascii_uppercase[:26]:\n",
    "    url = \"http://nameberry.com/search/boys_names/\" + n\n",
    "    response = urlopen(url)\n",
    "    html = response.read()\n",
    "    response.close()\n",
    "    soup = BeautifulSoup(html,\"html.parser\")\n",
    "    items = soup.findAll(\"li\", class_=\"name_in_list\")\n",
    "    for item in items:\n",
    "        if len(item.findAll(\"a\")) == 1:\n",
    "            boy_names.append(item.a.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(boy_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boy_names.sort()\n",
    "print(str(len(boy_names)) + \" names from \" + \\\n",
    "       boy_names[0] + \" to \" + boy_names[-1] + \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate we downloaded and parsed all the names, and to have a little fun, let's make up an official-sounding name for a childish Congressman."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "proper_person_name = \"\"\n",
    "for n in range(5):\n",
    "    proper_person_name += random.choice(boy_names) + \" \"\n",
    "proper_person_name = \"Congressman \" + proper_person_name[:-1] + \" XVI\" + \" PhD\"\n",
    "print(proper_person_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JSON API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import joshkey\n",
    "base_domain = \"http://words.bighugelabs.com/\"\n",
    "\n",
    "api_key =  joshkey.API # get your own damn key!\n",
    "word = \"hacker\"\n",
    "\n",
    "url = base_domain + \"api/2/\" + api_key + \"/\" + word + \"/json\"\n",
    "print(url)\n",
    "\n",
    "result = json.loads(urlopen(url).read().decode(\"UTF-8\")) # a dictionary!\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more fleshed-out example code, prints the output more cleanly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "base_domain = \"http://words.bighugelabs.com/\"\n",
    "api_key = \"483e281b60496d7961d852629799e733\"\n",
    "word = \"notebook\"\n",
    "print(\"Retrieving thesaurus entry for \\\"\" + word + \"\\\".\")\n",
    "url = base_domain + \"api/2/\" + api_key + \"/\" + word + \"/json\"\n",
    "try:\n",
    "    result = json.loads(urlopen(url).read().decode(\"UTF-8\")) # a dictionary!\n",
    "except:\n",
    "    print(\"Error - word probably not in thesaurus.\")\n",
    "    #sys.exit()\n",
    "for part_of_speech in result:\n",
    "    print(\"-\"*50)\n",
    "    print(\"These are the \" + part_of_speech + \" entries:\")\n",
    "    for key in [\"syn\", \"ant\", \"rel\"]:\n",
    "        try:\n",
    "            for synonym in result[part_of_speech][key]:\n",
    "                print(key + \" - \" + synonym)\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
